{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Spectrogram of a single audio file\n",
    "def save_spectrogram(curr_audio_path, curr_audio_name):\n",
    "    X, sr = librosa.load(curr_audio_path)  # librosa.load() returns an np array and sampling rate(by default 22050)\n",
    "    plt.specgram(X, Fs=22050)\n",
    "    plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.plot\n",
    "    plt.savefig('spectrograms/' + curr_audio_name,  bbox_inches= 'tight' , pad_inches = 0, dpi = 25)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Now we have all the spectrogram images in the 'sprectograms' folder, we need to make our datasets: X and Y\n",
    " The file is in the format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav (Now converted to an image)\n",
    " So to build 'Y' we can retrieve 'classID' from each of the file\n",
    " So we have to go to all the images, flatten each image into an array, \n",
    " append it to 'X' and store the corrsponding class values in 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "rootdir = 'spectrograms/'\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        curr_img_path = os.path.join(subdir, file)  # The path of current image file\n",
    "        curr_img_path = os.path.normpath(curr_img_path)  # To get '\\' instead of '/'\n",
    "        curr_img_name = os.path.splitext(file)[0]   # The name of current image file (withoud .png extension)\n",
    "        img = image.load_img(curr_img_path, target_size = (64, 64))  # Load the actual image file\n",
    "        img = image.img_to_array(img)        # Convert the loaded image file to the array\n",
    "        cls = int(curr_img_name.split('-')[1])\n",
    "        X.append(img)\n",
    "        Y.append(cls)\n",
    "print(len(X), len(Y))\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = to_categorical(Y)   # One hot encoding\n",
    "X.shape, type(X), Y.shape, type(Y), (Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we have our training and testing data, we can now train our CNN model using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_width = 64\n",
    "input_height = 64\n",
    "input_channels = 3\n",
    "input_shape = (input_width, input_height, input_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3),\n",
    "                 activation='relu', padding='same',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizers.rmsprop(lr=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1614 samples, validate on 539 samples\n",
      "Epoch 1/50\n",
      "1614/1614 [==============================] - 26s 16ms/step - loss: 89.7313 - accuracy: 0.2906 - val_loss: 2.8623 - val_accuracy: 0.2542\n",
      "Epoch 2/50\n",
      "1614/1614 [==============================] - 25s 15ms/step - loss: 1.9264 - accuracy: 0.4139 - val_loss: 1.5151 - val_accuracy: 0.4824\n",
      "Epoch 3/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 2.9276 - accuracy: 0.4752 - val_loss: 11.7508 - val_accuracy: 0.1224\n",
      "Epoch 4/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 1.5797 - accuracy: 0.5564 - val_loss: 1.4009 - val_accuracy: 0.5788\n",
      "Epoch 5/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 1.8994 - accuracy: 0.5496 - val_loss: 1.3078 - val_accuracy: 0.5807\n",
      "Epoch 6/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 1.2246 - accuracy: 0.6121 - val_loss: 1.1053 - val_accuracy: 0.6271\n",
      "Epoch 7/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.1840 - accuracy: 0.6543 - val_loss: 1.8752 - val_accuracy: 0.4861\n",
      "Epoch 8/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 1.0095 - accuracy: 0.7001 - val_loss: 1.0374 - val_accuracy: 0.6456\n",
      "Epoch 9/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.2010 - accuracy: 0.7261 - val_loss: 1.2172 - val_accuracy: 0.6809\n",
      "Epoch 10/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 1.7329 - accuracy: 0.7342 - val_loss: 1.0027 - val_accuracy: 0.7236\n",
      "Epoch 11/50\n",
      "1614/1614 [==============================] - 23s 15ms/step - loss: 1.4052 - accuracy: 0.8178 - val_loss: 0.9209 - val_accuracy: 0.7532\n",
      "Epoch 12/50\n",
      "1614/1614 [==============================] - 23s 15ms/step - loss: 0.6586 - accuracy: 0.8532 - val_loss: 0.9447 - val_accuracy: 0.7607\n",
      "Epoch 13/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 1.4246 - accuracy: 0.8048 - val_loss: 1.0141 - val_accuracy: 0.7421\n",
      "Epoch 14/50\n",
      "1614/1614 [==============================] - 23s 15ms/step - loss: 1.5453 - accuracy: 0.8575 - val_loss: 1.3799 - val_accuracy: 0.6716\n",
      "Epoch 15/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 0.3978 - accuracy: 0.8953 - val_loss: 1.0878 - val_accuracy: 0.7718\n",
      "Epoch 16/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.6078 - accuracy: 0.8854 - val_loss: 1.4875 - val_accuracy: 0.7421\n",
      "Epoch 17/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 2.1671 - accuracy: 0.8563 - val_loss: 1.9421 - val_accuracy: 0.7143\n",
      "Epoch 18/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 0.6544 - accuracy: 0.8804 - val_loss: 2.0458 - val_accuracy: 0.6883\n",
      "Epoch 19/50\n",
      "1614/1614 [==============================] - 23s 15ms/step - loss: 1.0516 - accuracy: 0.8563 - val_loss: 1.2373 - val_accuracy: 0.7922\n",
      "Epoch 20/50\n",
      "1614/1614 [==============================] - 26s 16ms/step - loss: 0.6616 - accuracy: 0.9089 - val_loss: 2.8997 - val_accuracy: 0.7273\n",
      "Epoch 21/50\n",
      "1614/1614 [==============================] - 27s 17ms/step - loss: 1.1743 - accuracy: 0.8711 - val_loss: 1.4307 - val_accuracy: 0.7570\n",
      "Epoch 22/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.4328 - accuracy: 0.9139 - val_loss: 3.8929 - val_accuracy: 0.6345\n",
      "Epoch 23/50\n",
      "1614/1614 [==============================] - 25s 15ms/step - loss: 1.3102 - accuracy: 0.8990 - val_loss: 1.3536 - val_accuracy: 0.8052\n",
      "Epoch 24/50\n",
      "1614/1614 [==============================] - 25s 15ms/step - loss: 1.0471 - accuracy: 0.9201 - val_loss: 4.2587 - val_accuracy: 0.6623\n",
      "Epoch 25/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.2808 - accuracy: 0.9542 - val_loss: 1.3952 - val_accuracy: 0.7922\n",
      "Epoch 26/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.3928 - accuracy: 0.9306 - val_loss: 1.3203 - val_accuracy: 0.8108\n",
      "Epoch 27/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.3723 - accuracy: 0.9126 - val_loss: 1.6217 - val_accuracy: 0.7848\n",
      "Epoch 28/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.0148 - accuracy: 0.9195 - val_loss: 2.7318 - val_accuracy: 0.6939\n",
      "Epoch 29/50\n",
      "1614/1614 [==============================] - 23s 14ms/step - loss: 0.2981 - accuracy: 0.9436 - val_loss: 2.8543 - val_accuracy: 0.6883\n",
      "Epoch 30/50\n",
      "1614/1614 [==============================] - 25s 15ms/step - loss: 0.7014 - accuracy: 0.9095 - val_loss: 1.4993 - val_accuracy: 0.7514\n",
      "Epoch 31/50\n",
      "1614/1614 [==============================] - 25s 15ms/step - loss: 0.3803 - accuracy: 0.9535 - val_loss: 2.1028 - val_accuracy: 0.7755\n",
      "Epoch 32/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.7407 - accuracy: 0.9418 - val_loss: 1.8817 - val_accuracy: 0.7866\n",
      "Epoch 33/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.9214 - accuracy: 0.9114 - val_loss: 20.0562 - val_accuracy: 0.4156\n",
      "Epoch 34/50\n",
      "1614/1614 [==============================] - 29s 18ms/step - loss: 0.8433 - accuracy: 0.9281 - val_loss: 1.9954 - val_accuracy: 0.7570\n",
      "Epoch 35/50\n",
      "1614/1614 [==============================] - 28s 17ms/step - loss: 0.9034 - accuracy: 0.9449 - val_loss: 1.6444 - val_accuracy: 0.7681\n",
      "Epoch 36/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.4834 - accuracy: 0.9542 - val_loss: 2.0029 - val_accuracy: 0.8108\n",
      "Epoch 37/50\n",
      "1614/1614 [==============================] - 25s 15ms/step - loss: 2.9100 - accuracy: 0.9207 - val_loss: 6.8787 - val_accuracy: 0.6549\n",
      "Epoch 38/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.6866 - accuracy: 0.9170 - val_loss: 2.3589 - val_accuracy: 0.8145\n",
      "Epoch 39/50\n",
      "1614/1614 [==============================] - 25s 16ms/step - loss: 0.5509 - accuracy: 0.9548 - val_loss: 2.1901 - val_accuracy: 0.7848\n",
      "Epoch 40/50\n",
      "1614/1614 [==============================] - 26s 16ms/step - loss: 0.4979 - accuracy: 0.9616 - val_loss: 6.8123 - val_accuracy: 0.6716\n",
      "Epoch 41/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.4504 - accuracy: 0.9263 - val_loss: 3.0869 - val_accuracy: 0.7588\n",
      "Epoch 42/50\n",
      "1614/1614 [==============================] - 25s 16ms/step - loss: 0.3839 - accuracy: 0.9603 - val_loss: 2.3345 - val_accuracy: 0.7904\n",
      "Epoch 43/50\n",
      "1614/1614 [==============================] - 25s 16ms/step - loss: 1.4505 - accuracy: 0.9566 - val_loss: 4.3926 - val_accuracy: 0.7291\n",
      "Epoch 44/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 1.0515 - accuracy: 0.9585 - val_loss: 3.1246 - val_accuracy: 0.7737\n",
      "Epoch 45/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.4576 - accuracy: 0.9498 - val_loss: 2.7517 - val_accuracy: 0.7959\n",
      "Epoch 46/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.9754 - accuracy: 0.9436 - val_loss: 4.3115 - val_accuracy: 0.7904\n",
      "Epoch 47/50\n",
      "1614/1614 [==============================] - 25s 15ms/step - loss: 0.3912 - accuracy: 0.9641 - val_loss: 2.4771 - val_accuracy: 0.7866\n",
      "Epoch 48/50\n",
      "1614/1614 [==============================] - 25s 16ms/step - loss: 0.7688 - accuracy: 0.9473 - val_loss: 3.0649 - val_accuracy: 0.7866\n",
      "Epoch 49/50\n",
      "1614/1614 [==============================] - 25s 16ms/step - loss: 0.4200 - accuracy: 0.9647 - val_loss: 2.7296 - val_accuracy: 0.7978\n",
      "Epoch 50/50\n",
      "1614/1614 [==============================] - 24s 15ms/step - loss: 0.8391 - accuracy: 0.9504 - val_loss: 2.8971 - val_accuracy: 0.8256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a722e47948>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/539 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8971323836491147, 0.8256029486656189]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model, so that we can use this trained model later also\n",
    "model.save('saved_models/UrbanSoundComplete.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on an audio file\n",
    "wav_path = 'testAudio0.wav'\n",
    "wav_name = os.path.splitext(wav_path)[0]\n",
    "save_spectrogram(wav_path, wav_name)\n",
    "png_path = 'spectrograms/' + wav_name + '.png'\n",
    "png_img = image.load_img(png_path, target_size = (64, 64))\n",
    "x = image.img_to_array(png_img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "pred = model.predict(x)\n",
    "class_idx = np.argmax(pred[0])\n",
    "predicted_class = class_names[class_idx]\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
